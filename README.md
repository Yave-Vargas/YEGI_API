# ğŸ“˜ YEGI API

Academic PDF Summarization API powered by **FastAPI + Ollama**

Note: This repository is for APIs only, but the complete application has a backend and a frontend.

**Version:** 0.2.0
**Authors:** YavÃ© Emmanuel Vargas MÃ¡rquez   (Backend)
             Giovanna Inosuli Campos Flores (Frontend)
---

# Overview

YEGI API allows users to:

* ğŸ“„ Upload academic PDF files (max 30MB)
* ğŸ§© Extract structured section headers
* ğŸ§  Generate scientific summaries using local LLMs (Ollama)
* ğŸ¯ Apply weighted emphasis to document sections
* âš™ Control inference parameters (temperature, top_p, etc.)

Designed for research environments and academic text processing.

---

# ğŸ§± Tech Stack

* **FastAPI** â€“ Web framework
* **Ollama** â€“ Local LLM runtime
* **PyMuPDF** â€“ PDF parsing
* **Langdetect** â€“ Language detection
* **python-dotenv** â€“ Environment configuration
* **Uvicorn** â€“ ASGI server

---

# ğŸ“‚ Project Structure

```
YEGI-API/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”‚   â””â”€â”€ summarizer.py
â”‚   â”‚   â””â”€â”€ router.py
â”‚   â”‚
â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â”œâ”€â”€ llm_controller.py
â”‚   â”‚   â”œâ”€â”€ extract_headers.py
â”‚   â”‚   â”œâ”€â”€ pdf_extractor.py
â”‚   â”‚   â””â”€â”€ text_preprocessor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ summarization_service.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

# âš™ï¸ Requirements

* Python 3.11+
* Ollama installed
* At least 8GB RAM recommended (for 3B models)

Install dependencies:

```bash
pip install -r requirements.txt
```

---

# ğŸ” Environment Configuration

Create a `.env` file in the project root:

```
FRONTEND_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
```

In production:

```
FRONTEND_ORIGINS=https://your-frontend-domain.com
```

CORS is restricted to these origins.

---

# ğŸ§ª Running Locally

Start Ollama:

```bash
ollama serve
```

Run the API:

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --limit-concurrency 2
```

Access interactive docs:

```
http://localhost:8000/docs
```

---

# ğŸ“Œ API Endpoints

## ğŸ“¦ Get Available Models

```
GET /api/models/
```

Returns available Ollama models.

---

## ğŸ“‘ Extract PDF Headers

```
POST /api/extract/headers
```

**Form-data:**

* `archivo_pdf` (file)

Response:

```json
{
  "total": 6,
  "headers": ["Introduction", "Methods", "Results"]
}
```

---

## ğŸ§  Summarize PDF

```
POST /api/summarizer/
```

**Form-data:**

| Field          | Type        |
| -------------- | ----------- |
| archivo_pdf    | file        |
| model          | string      |
| temperature    | float       |
| top_p          | float       |
| repeat_penalty | float       |
| repeat_last_n  | int         |
| num_predict    | int         |
| language       | string      |
| header_weights | JSON string |

---

# ğŸ›¡ Security & Stability Features

* âœ… 30MB file size limit
* âœ… Strict PDF validation
* âœ… Header weight normalization
* âœ… Automatic language detection
* âœ… Global error handler
* âœ… Restricted CORS
* âœ… No internal stack trace exposure
* âœ… Temporary file cleanup

---

# âš™ Performance Notes

Since the API runs local LLM models:

* Performance depends heavily on RAM and CPU.
* Recommended: limit concurrency using:

```bash
--limit-concurrency 2
```

* 3B models recommended for 8GB VPS environments.

---

# ğŸ³ Docker (Optional)

Build image:

```bash
docker build -t yegi-api .
```

Run container:

```bash
docker run -p 8000:8000 --env-file .env yegi-api
```

---

# âš ï¸ Limitations (v0.2.0)

* No authentication
* No rate limiting
* No structured logging yet
* Designed for single-node deployment

---

# ğŸ“„ License

Academic use â€“ Internal research project.

---

# ğŸ§  Architecture Notes

This project follows a layered structure:

* API Layer â†’ Request handling
* Controller Layer â†’ Business logic orchestration
* Service Layer â†’ Model interaction
* Core Layer â†’ Configuration

Designed for maintainability and future scaling.

---
