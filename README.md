# ğŸ“˜ YEGI API

Academic PDF Summarization API powered by **FastAPI + Ollama**

> âš ï¸ This repository contains the backend API only.
> The complete application includes a separate frontend service.

**Version:** 0.2.0
**Authors:**

* YavÃ© Emmanuel Vargas MÃ¡rquez (Backend)
* Giovanna Inosuli Campos Flores (Frontend)

---

# ğŸš€ Overview

YEGI API allows users to:

* ğŸ“„ Upload academic PDF files (max 30MB)
* ğŸ§© Extract structured section headers
* ğŸ§  Generate scientific summaries using local LLMs (Ollama)
* ğŸ¯ Apply weighted emphasis to document sections
* âš™ Control inference parameters (temperature, top_p, etc.)

Designed for research environments and academic text processing.

---

# ğŸ§± Tech Stack

* **FastAPI** â€“ Web framework
* **Ollama** â€“ Local LLM runtime
* **PyMuPDF** â€“ PDF parsing
* **Langdetect** â€“ Language detection
* **python-dotenv** â€“ Environment configuration
* **Uvicorn** â€“ ASGI server
* **Docker & Docker Compose** â€“ Containerized deployment

---

# ğŸ“‚ Project Structure

```
YEGI-API/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â””â”€â”€ router.py
â”‚   â”‚
â”‚   â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ core/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .dockerignore
â””â”€â”€ README.md
```

---

# âš™ï¸ Requirements

If running without Docker:

* Python 3.11+
* Ollama installed
* 8GB RAM recommended (for 3B models)

Install dependencies:

```bash
pip install -r requirements.txt
```

---

# ğŸ” Environment Configuration

Create a `.env` file in the project root:

```
FRONTEND_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
OLLAMA_HOST=http://ollama:11434
```

In production:

```
FRONTEND_ORIGINS=https://your-frontend-domain.com
```

CORS is restricted to these origins.

---

# ğŸ³ Quick Start (Recommended)

### 1ï¸âƒ£ Build & Start Services

```bash
docker compose up --build -d
```

This will start:

* `yegi_api`
* `yegi_ollama`

---

### 2ï¸âƒ£ Pull LLM Model (First Time Only)

```bash
docker exec -it yegi_ollama ollama pull llama3.2:3b
```

---

### 3ï¸âƒ£ Access API Docs

```
http://localhost:8000/docs
```
---

Perfecto ğŸ”¥ â€” eso es importante para que tu README quede completo y profesional.

Te agrego una secciÃ³n clara, lista para copiar y pegar dentro de tu README, justo despuÃ©s de **â€œPull LLM Model (First Time Only)â€**.

---

# â• Adding More Models to Ollama

YEGI API supports any model installed in the Ollama container.

---

## ğŸ” 1ï¸âƒ£ List Available Remote Models

You can browse models from the official Ollama library:

ğŸ‘‰ [https://ollama.com](https://ollama.com)

---

## ğŸ“¥ 2ï¸âƒ£ Pull a New Model (Docker)

Run inside the Ollama container:

```bash
docker exec -it yegi_ollama ollama pull mistral:7b
```

Example models:

```bash
docker exec -it yegi_ollama ollama pull llama3.2:1b
docker exec -it yegi_ollama ollama pull llama3.2:3b
docker exec -it yegi_ollama ollama pull mistral:7b
docker exec -it yegi_ollama ollama pull phi3:mini
```

---

## ğŸ–¥ 3ï¸âƒ£ Pull Model (Without Docker)

If running locally:

```bash
ollama pull mistral:7b
```

---

## ğŸ“‹ 4ï¸âƒ£ Verify Installed Models

Docker:

```bash
docker exec -it yegi_ollama ollama list
```

Local:

```bash
ollama list
```

---

## âš ï¸ Resource Considerations

Model size impacts RAM usage:

| Model Size | Recommended RAM |
| ---------- | --------------- |
| 1B         | 4GB             |
| 3B         | 8GB             |
| 7B         | 16GB            |
| 13B+       | 32GB+           |

---

# ğŸ§ª Running Locally (Without Docker)

Start Ollama:

```bash
ollama serve
```

Run API:

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

---

# ğŸ“Œ API Endpoints

## ğŸ“¦ GET /api/models/

Returns available Ollama models.

---

## ğŸ“‘ POST /api/extract/headers

Form-data:

* `archivo_pdf` (file)

---

## ğŸ§  POST /api/summarizer/

Form-data:

| Field          | Type        |
| -------------- | ----------- |
| archivo_pdf    | file        |
| model          | string      |
| temperature    | float       |
| top_p          | float       |
| repeat_penalty | float       |
| repeat_last_n  | int         |
| num_predict    | int         |
| language       | string      |
| header_weights | JSON string |

Example:

```bash
-F 'header_weights={"Introduction":40,"Results":60}'
```

---

# ğŸ›¡ Security & Stability

* 30MB file size limit
* Strict PDF validation
* Header weight normalization
* Automatic language verification
* Global error handler
* Restricted CORS
* No internal stack traces exposed
* Temporary file cleanup

---

# âš™ Performance Considerations

Since this API runs local LLM models:

* Performance depends on CPU and RAM
* Recommended concurrency limit:

```bash
--limit-concurrency 2
```

* 3B models recommended for 8GB VPS
* For production, consider vertical scaling or GPU acceleration

---

# âš ï¸ Limitations (v0.2.0)

* No authentication
* No rate limiting
* No persistent storage
* Single-node deployment
* No background job queue

---

# ğŸ§  Architecture

Layered structure:

* API Layer â†’ HTTP handling
* Controller Layer â†’ Business logic
* Service Layer â†’ LLM interaction
* Core Layer â†’ Configuration

Designed for maintainability and future scaling.

---

# ğŸ“„ License

Academic use â€“ Internal research project.

---
